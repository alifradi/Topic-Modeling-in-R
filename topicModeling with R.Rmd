---
title: "Topic modeling"
author: "Ali Frady"
date: "8/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries}
library(tidytext)
library(quanteda)
library(ggplot2)
library(tm)
library(topicmodels)
library(tidyverse)
library(wordcloud)
library(gutenbergr)
```

Simple LDA model
In this exercise you will work through all steps involved in making a topic model analysis. For simplicity, we will use the tiny corpus of five sentences/documents that you have seen earlier.

You are given data frame corpus with two columns: id is the document id, and text is the text of the document.


```{r exercice 1}
corpus <- readRDS('data/corpus.rds')
dtm <- corpus %>% 
   unnest_tokens(input=text, output=word) %>% 
   count(id, word) %>% 
   cast_dtm(document=id, term=word, value=n)

# Run the LDA for two topics
mod <- LDA(x=dtm, k=2, method="Gibbs",control=list(alpha=1, delta=0.1, seed=10005))

# Retrieve the probabilities of word `will` belonging to topics 1 and 2
tidy(mod, matrix="beta") %>%
  filter(term == "will")

# Make a stacked column chart showing the probabilities of documents belonging to topics
tidy(mod, matrix="gamma") %>% 
  mutate(topic = as.factor(topic)) %>% 
  ggplot(aes(x=document, y=gamma)) + 
  geom_col(aes(fill=topic))

```
Effect of argument alpha
In this exercise you will compare how the quality of model's fit to data varies with argument alpha

```{r Effect_of_argument_alpha}
# Fit LDA topic model using Gibbs sampling for 2 topics
mod1 <- LDA(x=dtm, k=2, method="Gibbs",
           control=list(alpha=1, seed=10005, thin=1))

# Display the probabilities of topics in documents side by side
tidy(mod1, matrix = 'gamma') %>% spread(topic, gamma)

# Fit LDA topic model using Gibbs sampling for 2 topics
mod2 <- LDA(x=dtm, k=2, method="Gibbs",
           control=list(alpha=25, seed=10005, thin=1))

# Display the probabilities of topics in documents side by side
tidy(mod2, matrix = 'gamma') %>% spread(topic, gamma)
```
Making a dtm refresher

You are given a table corpus: column text contains the documents, column id - the document ids. Your task is to make a document-term matrix. You've done this before, so it should be easy.

```{r Making_a_dtm_refresher}
# Create the document-term matrix
dtm <- corpus %>%
  unnest_tokens(output=word, input=text) %>%
  count(id, word) %>%
  cast_dtm(document=id, term=word, value=n)

# Display dtm as a matrix
as.matrix(dtm)
```

Removing stopwords
It takes only one new line of code to remove the stopwords. Fill in the function names to make the code work.

```{r Removing_stopwords}
# Create the document-term matrix with stop words removed
dtm <- corpus %>%
  unnest_tokens(output=word, input=text) %>%
  anti_join(stop_words) %>% 
  count(id, word) %>%
  cast_dtm(document=id, term=word, value=n)

# Display the matrix
as.matrix(dtm)
```
Keeping the needed words
You are given the table with documents corpus and the table dictionary with one column - word, - containing the words we want to keep in the document-term matrix. Use inner_join to create a document-term matrix with the needed words.
```{r Keeping the needed words}
dictionary <-data.frame(word=c( "bank", "fines", "loans", "pay", "new", "opened", "restaurant"))
# Perform inner_join with the dictionary table
dtm <- corpus %>%
  unnest_tokens(output=word, input=text) %>%
  inner_join(dictionary) %>% 
  count(id, word) %>%
  cast_dtm(document=id, term=word, value=n)

# Display the summary of dtm
as.matrix(dtm)
```


Wordcloud of term frequency
You are given table corpus containing the "toy" corpus with five sentences/documents. You will practice modifying the word cloud to make it more interesting.

```{r Wordcloud of term frequency}
# Generate the counts of words in the corpus
word_frequencies <- corpus %>% 
  unnest_tokens(input=text, output=word) %>%
  count(word)

# Create a wordcloud
wordcloud(words=word_frequencies$word, 
          freq=word_frequencies$n,
          min.freq=1,
          max.words=10,
          colors=c("DarkOrange", "Blue"),
          random.order=FALSE,
          random.color=FALSE)
```


LDA model fitting - first iteration
This exercise covers the steps from making a document-term matrix to fitting a topic model and examining the terms in topics.

You are given a table history with two columns: chapter for chapter number, and text for chapter text.


```{r LDA model fitting - first iteration}
history <- readRDS('data/history.rds') %>% filter(!is.na(text))
# Construct a document-term matrix
dtm <- history %>% 
	unnest_tokens(input=text, output=word) %>% 
    anti_join(stop_words) %>% 
    count(chapter, word) %>% 
    cast_dtm(document=chapter, term=word, value=n)

# Insert the missing arguments
mod <- LDA(x=dtm, k=4, method="Gibbs", 
           control=list(alpha=1, seed=10005))

# Display top 15 words of each topic
terms(mod, k=15)
```
Capturing the actions - dtm with verbs
In this exercise you will construct the dtm that will consist entirely of verbs, and then re-run the LDA algorithm.

You are given the dataframe verbs containing present and past tense forms of English verbs.

```{r Capturing the actions - dtm with verbs}
# Display the structure of the verbs dataframe
verbs <- readRDS('data/verbs.rds')
str(verbs)

# Construct a document-term matrix
dtm <- history %>% 
	unnest_tokens(input=text, output=word) %>% 
    inner_join(verbs, by=c("word"="Past")) %>% 
    count(chapter, word) %>% 
    cast_dtm(document=chapter, term=word, value=n)

# Fit LDA for four topics
mod <- LDA(x=dtm, k=4, method="Gibbs",
          control=list(alpha=1, seed=10005))

# Display top 25 words from each topic
terms(mod, k=25)
```

Making a chart
Variable mod contains the LDA model that you fitted in the previous exercise. All necessary libraries have already been loaded for you.

```{r Making a chart}
# Extract matrix gamma and plot it
tidy(mod, "gamma") %>% 
	mutate(document=as.numeric(document)) %>% 
	ggplot(aes(x=document, y=gamma)) + 
	geom_col(aes(fill=factor(topic)))

# Display the words whose probability is above the threshold
terms(mod, threshold=0.0075 )


# Extract matrix gamma and plot it
tidy(mod, "gamma") %>% 
	mutate(document=as.numeric(document)) %>% 
	ggplot(aes(x=document, y=gamma)) + 
	geom_line(aes(color=factor(topic))) + 
    labs(x="Chapter", y="Topic probability") +
	scale_color_manual(values=brewer.pal(n=4, "Set1"), name="Topic")
```

**Use wordclouds**
The flaw of function terms() is that it does not display the absolute value of the word probability. By comparison, wordclouds can convey that information through font size. In this exercise you will make wordclouds for topics found in the text on Byzantine Empire.

You are given the object with LDA model mod. You're going to complete the script to draw four wordlcouds, one for each topic. You will be able to cycle through them in the output window.


```{r Making a chartUse wordclouds}
# Display wordclouds one at a time
for (j in 1:4) {
  # Generate a table with word frequences for topic j
  word_frequencies <- tidy(mod, matrix="beta") %>% 
    mutate(n = trunc(beta * 10000)) %>% 
    filter(topic == j)

  # Display word cloud
  wordcloud(words = word_frequencies$term, 
            freq = word_frequencies$n,
            max.words = 20,
            scale = c(3, 0.5),
            colors = c("DarkOrange", "CornflowerBlue", "DarkRed"), 
            rot.per = 0.3)
}
```

Same k, different alpha
You are given a document-term matrix dtm describing the five-sentence corpus of two topics. You will re-run the LDA algorithm, changing the value of alpha, and compare the outcomes.

When alpha is NULL, the package sets alpha = 50/k which in our case is 25. This favors topic proportions that are nearly equal to each other

```{r Same k, different alpha}
dtm <- corpus %>%
  unnest_tokens(output=word, input=text) %>%
  inner_join(dictionary) %>% 
  count(id, word) %>%
  cast_dtm(document=id, term=word, value=n)

# Fit a topic model using LDA with Gibbs sampling
mod = LDA(x=dtm, k=2, method="Gibbs", 
          control=list(iter=500, thin=1,
                      seed = 12345,
                      alpha=NULL))

# Display topic prevalance in documents as a table
tidy(mod, "gamma") %>% spread(topic, gamma)


# Fit a topic model using LDA with Gibbs sampling
mod = LDA(x=dtm, k=2, method="Gibbs", 
          control=list(iter=500, thin=1,
                      seed = 12345,
                      alpha=2))

# Display topic prevalance in documents as a table
tidy(mod, "gamma") %>% spread(topic, gamma)



# Fit a topic model using LDA with Gibbs sampling
mod = LDA(x=dtm, k=2, method="Gibbs", 
          control=list(iter=500, thin=1,
                      seed = 12345,
                      alpha=1))

# Display topic prevalance in documents as a table
tidy(mod, "gamma") %>% spread(topic, gamma)


# Fit a topic model using LDA with Gibbs sampling
mod = LDA(x=dtm, k=2, method="Gibbs", 
          control=list(iter=500, thin=1,
                      seed = 12345,
                      alpha=0.5))

# Display topic prevalance in documents as a table
tidy(mod, "gamma") %>% spread(topic, gamma)
```

Probabilities of words in topics
Parameter alpha determines the values of probabilities that a document belongs to a topic. Parameter delta does the same for probability distribution of words over topics. By default, delta is set to 0.1. You will fit a model with a different delta and make a plot of results.
```{r Probabilities of words in topics}
# Fit the model for delta = 0.1
mod <- LDA(x=dtm, k=2, method="Gibbs",
         control=list(iter=500, seed=12345, alpha=1, delta=0.1))

# Define which words we want to examine
my_terms = c("loans", "bank", "opened", "pay", "restaurant", "you")

# Make a tidy table
t <- tidy(mod, "beta") %>% filter(term %in% my_terms)

# Make a stacked column chart of word probabilities
ggplot(t, aes(x=term, y=beta)) + geom_col(aes(fill=factor(topic))) +
  theme(axis.text.x=element_text(angle=90))



# Fit the model for delta = 0.5
mod <- LDA(x=dtm, k=2, method="Gibbs",
         control=list(iter=500, seed=12345, alpha=1, delta=0.5))

# Define which words we want to examine
my_terms = c("loans", "bank", "opened", "pay", "restaurant", "you")

# Make a tidy table
t <- tidy(mod, "beta") %>% filter(term %in% my_terms)

# Make a stacked column chart
ggplot(t, aes(x=term, y=beta)) + geom_col(aes(fill=factor(topic))) +
  theme(axis.text.x=element_text(angle=90))
```


Regex patterns for entity matching
Vector text contains text of chapters of The Byzantine Empire by Charles Oman. You will experiment with the regex patterns for entity matching.
```{r Regex patterns for entity matching}
text <- readRDS('data/text.rds')
# Regex pattern for an entity and word context
p1 <- "( [a-z]+){2}( (St[.] )?[A-Z][a-z]+)+( [a-z]+){2}"

# Obtain the regex match object from gregexpr
m <- gregexpr(p1, text)

# Get the matches and flatten the list
v <- unlist(regmatches(text, m))

# Find the number of elements in the vector
length(v)


# Regex pattern for an entity and word context
p2 <- "( [a-z]+){2}( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+( [a-z]+){2}"

# Obtain the regex match object from gregexpr
m <- gregexpr(p2, text)

# Get the matches and flatten the list
v <- unlist(regmatches(text, m))

```

Making a corpus
You are given the pattern entity_pattern for the named entity. Vector v contains strings with named entities and two words to the left and to the right of the entity. You are going to make a table containing entity and its context as two columns.
```{r Regex patterns for entity matching}
# Print out contents of the `entity_pattern`
entity_pattern = "( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+"

# Remove the named entity from text
v2 <- gsub(entity_pattern, "", v)

# Display the head of v2
head(v2)


# Remove the named entity
v2 <- gsub(entity_pattern, "", v)

# Pattern for inserting suffixes
p <- "\\1_L1 \\2_L2 \\3_R1 \\4_R2"

# Add suffixes to words
context <- gsub("([a-z]+) ([a-z]+) ([a-z]+) ([a-z]+)", p, v2)


# Extract named entity and use it as document ID
re_match <-  gregexpr(entity_pattern, v)
doc_id <- unlist(regmatches(v, re_match))

# Make a data frame with columns doc_id and text
corpus <- data.frame(doc_id = doc_id, text = context, stringsAsFactors = F)

```
**From dtm to topic model**
You are given data frame corpus. Each row corresponds to one occurrence of a named entity. Column doc_id contains the entity, text - the context words with suffixes. You will build a document-term matrix and will fit a topic model.

```{r From dtm to topic model}
# Summarize the text to produce a document for each doc_id
corpus2 <- corpus %>% group_by(doc_id) %>% 
	summarize(doc = paste(text, collapse = " "))
corpus2

# Make a document-term matrix
dtm <- corpus2 %>% unnest_tokens(input = doc, output = word) %>% 
	count(doc_id, word) %>% 
	cast_dtm(document = doc_id, term = word, value = n)

# Fit an LDA model for 3 topics
mod <- LDA(x = dtm, k = 3, method = "Gibbs", 
          control=list(alpha = 1, seed = 12345, iter = 1000, thin = 1))
# Create a table with probabilities of topics in documents
topics <- tidy(mod, matrix="gamma") %>% 
	spread(topic, gamma)
```

Train a topic model
You are given a table corpus2: column doc_id contains the named entity, column doc contains context words of entities. You will take a random sample of documents, construct a training dataset and use it to make a topic model.

```{r Train a topic model}
# Set random seed for reproducability
set.seed(12345)

# Take a sample of 20 random integers, without replacement
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE)

# Generate a document-term matrix
train_dtm <- corpus2[-r, ] %>% unnest_tokens(input=doc, output=word) %>% 
  count(doc_id, word) %>% 
  cast_dtm(document=doc_id, term=word, value=n)

# Fit an LDA topic model for k=3
train_mod <- LDA(x=train_dtm, k=3, method="Gibbs",
                control=list(alpha=1, seed=10001,
                             iter=1000, thin=1))
```

Align corpus
You have LDA model object train_mod and table corpus2 with initial data. You will need to align the corpus of the test records and make a document-term matrix for testing.

```{r Align corpus}
# Get the test row indices
set.seed(12345)
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE)

# Extract the vocabulary of the training model
model_vocab <- tidy(train_mod, matrix="beta") %>% 
  select(term) %>% distinct()

# Create a table of counts with aligned vocabularies
test_table <- corpus2[r, ] %>% unnest_tokens(input=doc, output=word) %>% 
  count(doc_id, word) %>%
  right_join(model_vocab, by=c("word"="term"))

# Prepare a document-term matrix
test_dtm <- test_table %>% 
  arrange(desc(doc_id)) %>% 
  mutate(doc_id = ifelse(is.na(doc_id), first(doc_id), doc_id),
         n = ifelse(is.na(n), 0, n)) %>% 
  cast_dtm(document=doc_id, term=word, value=n)
```


Classify test data
You have a data object train_mod with an LDA model, and a document-term matrix test_dtm with data for the test cases. Now you can see how well (or how poorly) our classifier performs.

```{r Classify test data}
# Obtain posterior probabilities for test documents
results <- posterior(object=train_mod, newdata=test_dtm)

# Display the matrix with topic probabilities
results$topics
```

